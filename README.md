# -基于Spark平台利用信息熵实现中文分词-Demo

这是一个小的Demo尝试，实现中文分词

1.平台选择的是Spark,主要是借助此次练习熟悉Spark相关的知识；

2.中文分词，用的算法思路比较简单，从信息论的角度来解决中文分词的问题。

根据大数定律，我们知道，当样本越多时，其分布越接近真实的分布，样本发生的概率越接近于真实值。

分词算法过程：

1.利用Spark将文本切分汉字组合，其中每个组合汉字最大个数限制为4

2.计算每个汉字组合在文本中出现的频率

3.计算经验信息熵H(p)=-p*log(p)

4.过滤到信息熵比较低的组合

5.对得到的分词结果（短语，频数）按照频数进行排序，取Top50

6.生成词云
